{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "therapeutic-window",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "to_img = T.ToPILImage()\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "micro-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 128\n",
    "UPSCALE_FACTOR = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        mask = torch.rand(tensor.size()) > 0.9\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + mask * noise\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize, GaussianBlur, Grayscale\n",
    "\n",
    "def is_image_file(filename):\n",
    "     return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    " return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "def train_hr_transform(crop_size):\n",
    " return Compose([\n",
    "     RandomCrop(crop_size),\n",
    "     ToTensor(),\n",
    " ])\n",
    "\n",
    "\n",
    "def train_lr_transform(crop_size, upscale_factor):\n",
    " return Compose([\n",
    "     AddGaussianNoise(),\n",
    "     ToPILImage(),\n",
    "#      GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "     Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
    "     ToTensor()\n",
    " ])\n",
    "\n",
    "\n",
    "def display_transform():\n",
    " return Compose([\n",
    "     ToPILImage(),\n",
    "     Resize(400),\n",
    "     CenterCrop(400),\n",
    "     ToTensor()\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intended-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetFromFolder(Dataset):\n",
    "     def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "         super(TrainDatasetFromFolder, self).__init__()\n",
    "         self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "         crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
    "         self.hr_transform = train_hr_transform(crop_size)\n",
    "         self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
    "\n",
    "     def __getitem__(self, index):\n",
    "         hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
    "         lr_image = self.lr_transform(hr_image)\n",
    "         return lr_image, hr_image\n",
    "\n",
    "     def __len__(self):\n",
    "         return len(self.image_filenames)\n",
    "        \n",
    "        \n",
    "class ValDatasetFromFolder(Dataset):\n",
    "     def __init__(self, dataset_dir, upscale_factor):\n",
    "         super(ValDatasetFromFolder, self).__init__()\n",
    "         self.upscale_factor = upscale_factor\n",
    "         self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "\n",
    "     def __getitem__(self, index):\n",
    "         hr_image = Image.open(self.image_filenames[index])\n",
    "         w, h = hr_image.size\n",
    "         crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n",
    "         lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n",
    "         hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
    "         hr_image = CenterCrop(crop_size)(hr_image)\n",
    "         lr_image = lr_scale(hr_image)\n",
    "         hr_restore_img = hr_scale(lr_image)\n",
    "         return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
    "\n",
    "     def __len__(self):\n",
    "         return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = TrainDatasetFromFolder('data/DIV2K/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "val_set = ValDatasetFromFolder('/home/ubuntu/noisy-superres/data/DIV2K/val', upscale_factor=UPSCALE_FACTOR)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=0, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fewer-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_train = iter(train_loader)\n",
    "# iter_val = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "racial-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_img, lr_img_rescaled, gt_img = val_set[0]\n",
    "# print(lr_img.shape)\n",
    "# print(lr_img_rescaled.shape)\n",
    "# print(gt_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "centered-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_img(gt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weekly-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_img(lr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "posted-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_img(lr_img_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hundred-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./SRGAN\")\n",
    "sys.path.append(\"./SRGAN-PyTorch\")\n",
    "import srgan_pytorch\n",
    "import srgan_pytorch.models as srgan_models\n",
    "from srgan_pytorch.utils.estimate import iqa\n",
    "\n",
    "sys.path.append(\"./ESRGAN-PyTorch\")\n",
    "import esrgan_pytorch\n",
    "import esrgan_pytorch.models as esrgan_models\n",
    "\n",
    "\n",
    "sys.path.append(\"./DIDN\")\n",
    "import color_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "enabling-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-makeup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "studied-branch",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-aircraft",
   "metadata": {},
   "source": [
    "```\n",
    "srgan one network (srgan_one)\n",
    "esrgan one network (esrgan_one)\n",
    "\n",
    "denoise first\n",
    "cv2 (cv2_lr_denoised) -> srgan (cv2_srgan)\n",
    "cv2 (cv2_lr_denoised) -> esrgan (cv2_esrgan)\n",
    "didn (didn_lr_denoised) -> srgan (didn_srgan)\n",
    "didn (didn_lr_denoised) -> esrgan (didn_esrgan)\n",
    "\n",
    "superres first\n",
    "srgan (srgan_hr_noisy)-> cv2 (srgan_cv2)\n",
    "esrgan (esrgan_hr_noisy)-> cv2 (esrgan_cv2)\n",
    "srgan (srgan_hr_noisy) -> didn (srgan_didn)\n",
    "esrgan (esrgan_hr_noisy) -> didn (esrgan_didn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recognized-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"/home/ubuntu/noisy-superres/data/DIV2K/vis_images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-trainer",
   "metadata": {},
   "source": [
    "# Get lr, lr_noisy, and gt images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "legendary-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0, 30):\n",
    "    if count >= 10:\n",
    "        break\n",
    "    lr_img, lr_img_rescaled, gt_img = val_set[i + 30]\n",
    "    if lr_img.shape[-1] < 328:\n",
    "        continue\n",
    "    noisy_lr_img = AddGaussianNoise()(lr_img)\n",
    "    \n",
    "    to_img(lr_img).save(basepath + \"lr/\" + \"img\"+str(count)+\".png\")\n",
    "    torch.save(lr_img, basepath + \"lr/\" + \"img\"+str(count)+\".pt\")\n",
    "    \n",
    "    to_img(noisy_lr_img).save(basepath + \"lr_noisy/\" + \"img\"+str(count)+\".png\")\n",
    "    torch.save(noisy_lr_img, basepath + \"lr_noisy/\" + \"img\"+str(count)+\".pt\")\n",
    "    \n",
    "    to_img(gt_img).save(basepath + \"gt/\" + \"img\"+str(count)+\".png\")\n",
    "    torch.save(gt_img, basepath + \"gt/\" + \"img\"+str(count)+\".pt\")\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-defeat",
   "metadata": {},
   "source": [
    "# Get cv2_lr_denoised images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "herbal-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        filename = filename[:-3]\n",
    "        cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "#         print(filename)\n",
    "#         img = ToTensor()(Image.open(cur_filepath))\n",
    "        img = torch.load(cur_filepath)\n",
    "        img = img.numpy()\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "        img = cv2.fastNlMeansDenoisingColored((img * 255).astype('uint8'), None,10,10,7,21)\n",
    "        img = torch.from_numpy(np.moveaxis(img, -1, 0).astype('float32'))\n",
    "        img = img / 255\n",
    "        torch.save(img, basepath + \"cv2_lr_denoised/\" + filename+\".pt\")\n",
    "        to_img(img).save(basepath+\"cv2_lr_denoised/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-questionnaire",
   "metadata": {},
   "source": [
    "# Get didn_lr_denoised images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emotional-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.activation.PReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'color_model._Residual_Block' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.pixelshuffle.PixelShuffle' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "denoise_model = color_model._NetG()\n",
    "checkpoint = torch.load('DIDN/checkpoint/color_model.pth', map_location=lambda storage, loc: storage)\n",
    "denoise_model.load_state_dict(checkpoint['model'].state_dict())\n",
    "denoise_model = denoise_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bigger-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 336, 336]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c0146cab1a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"didn_lr_denoised/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"didn_lr_denoised/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "            dim = img.shape[-1] // 8 * 8\n",
    "            img = denoise_model(img[:,:dim, :dim].unsqueeze(0).to(device))\n",
    "            img = img[0].clip(0, 1)\n",
    "            torch.save(img, basepath + \"didn_lr_denoised/\" + filename+\".pt\")\n",
    "            to_img(img).save(basepath+\"didn_lr_denoised/\"+filename[:-3]+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-times",
   "metadata": {},
   "source": [
    "# Get srgan_hr_noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "alone-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"SRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "srgan = srgan_models.__dict__[\"srgan\"]()\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "srgan.load_state_dict(state_dict)\n",
    "srgan.eval()\n",
    "srgan = srgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "median-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1356, 1356])\n",
      "torch.Size([3, 1620, 1620])\n",
      "torch.Size([3, 1356, 1356])\n",
      "torch.Size([3, 1356, 1356])\n",
      "torch.Size([3, 1356, 1356])\n",
      "torch.Size([3, 1872, 1872])\n",
      "torch.Size([3, 1356, 1356])\n",
      "torch.Size([3, 1596, 1596])\n",
      "torch.Size([3, 1536, 1536])\n",
      "torch.Size([3, 1500, 1500])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "            img = srgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"srgan_hr_noisy/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-butter",
   "metadata": {},
   "source": [
    "# Get esrgan_hr_noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "responsible-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ESRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "esrgan = esrgan_models.__dict__[\"esrgan16\"]()\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "esrgan.load_state_dict(state_dict)\n",
    "esrgan.eval()\n",
    "esrgan = esrgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "parental-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "            img = esrgan(img.unsqueeze(0).to(device))[0]\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "            torch.save(img, basepath + \"esrgan_hr_noisy/\" + filename+\".pt\")\n",
    "            to_img(img).save(basepath+\"esrgan_hr_noisy/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-saying",
   "metadata": {},
   "source": [
    "# SRGAN one network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wrong-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"SRGAN-PyTorch/weights/Generator_best.pth\"\n",
    "model_path = \"SRGAN-PyTorch/best_weights/noisy/GAN.pth\"\n",
    "noisy_srgan = srgan_models.__dict__[\"srgan\"]()\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "noisy_srgan.load_state_dict(state_dict.get('state_dict', state_dict))\n",
    "noisy_srgan.eval()\n",
    "noisy_srgan = noisy_srgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ethical-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "            img = noisy_srgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"srgan_one/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"srgan_one/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-bread",
   "metadata": {},
   "source": [
    "# ESRGAN one network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "violent-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ESRGAN-PyTorch/best_weights/noisy/GAN.pth\"\n",
    "noisy_esrgan = esrgan_models.__dict__[\"esrgan16\"]()\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "noisy_esrgan.load_state_dict(state_dict.get('state_dict', state_dict))\n",
    "noisy_esrgan.eval()\n",
    "noisy_esrgan = noisy_esrgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sufficient-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"lr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"lr_noisy/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "            img = noisy_esrgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"srgan_one/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"srgan_one/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-hybrid",
   "metadata": {},
   "source": [
    "# cv2_srgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "impressed-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"SRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "srgan = srgan_models.__dict__[\"srgan\"]()\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "srgan.load_state_dict(state_dict)\n",
    "srgan.eval()\n",
    "srgan = srgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "found-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"cv2_lr_denoised/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"cv2_lr_denoised/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "#             print(img)\n",
    "#             raise\n",
    "            img = srgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"cv2_srgan/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"cv2_srgan/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-andrew",
   "metadata": {},
   "source": [
    "# cv2_esrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "subsequent-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ESRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "esrgan = esrgan_models.__dict__[\"esrgan16\"]()\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "esrgan.load_state_dict(state_dict)\n",
    "esrgan.eval()\n",
    "esrgan = esrgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "catholic-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"cv2_lr_denoised/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"cv2_lr_denoised/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "#             print(img)\n",
    "#             raise\n",
    "            img = esrgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"cv2_esrgan/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"cv2_esrgan/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-bread",
   "metadata": {},
   "source": [
    "# didn_srgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "virtual-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"SRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "srgan = srgan_models.__dict__[\"srgan\"]()\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "srgan.load_state_dict(state_dict)\n",
    "srgan.eval()\n",
    "srgan = srgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "statutory-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"didn_lr_denoised/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"didn_lr_denoised/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "#             print(img)\n",
    "#             raise\n",
    "            img = srgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"didn_srgan/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"didn_srgan/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-logan",
   "metadata": {},
   "source": [
    "# didn_esrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "tutorial-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ESRGAN-PyTorch/best_weights/vanilla/GAN.pth\"\n",
    "esrgan = esrgan_models.__dict__[\"esrgan16\"]()\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "esrgan.load_state_dict(state_dict)\n",
    "esrgan.eval()\n",
    "esrgan = esrgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "affected-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"didn_lr_denoised/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"didn_lr_denoised/\", filename+\".pt\")\n",
    "            img = torch.load(cur_filepath)\n",
    "#             print(img)\n",
    "#             raise\n",
    "            img = esrgan(img.unsqueeze(0).to(device))[0]\n",
    "#             img = img.cpu().detach().numpy().clip(0,1)\n",
    "            img = img.cpu().detach().clip(0,1)\n",
    "#             img = torch.from_numpy(np.moveaxis(img, 0, -1))\n",
    "#             print(img.shape)\n",
    "#             raise\n",
    "            torch.save(img, basepath + \"didn_esrgan/\" + filename+\".pt\")\n",
    "#             to_img(np.uint8(img * 255)).save(basepath+\"srgan_hr_noisy/\"+filename+\".png\")\n",
    "            to_img(img).save(basepath+\"didn_esrgan/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-grain",
   "metadata": {},
   "source": [
    "# srgan_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bizarre-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(basepath+\"srgan_hr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        filename = filename[:-3]\n",
    "        cur_filepath = os.path.join(basepath+\"srgan_hr_noisy/\", filename+\".pt\")\n",
    "#         print(filename)\n",
    "#         img = ToTensor()(Image.open(cur_filepath))\n",
    "        img = torch.load(cur_filepath)\n",
    "        img = img.numpy()\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "        img = cv2.fastNlMeansDenoisingColored((img * 255).astype('uint8'), None,10,10,7,21)\n",
    "        img = torch.from_numpy(np.moveaxis(img, -1, 0).astype('float32'))\n",
    "        img = img / 255\n",
    "        torch.save(img, basepath + \"srgan_cv2/\" + filename+\".pt\")\n",
    "        to_img(img).save(basepath+\"srgan_cv2/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-sunset",
   "metadata": {},
   "source": [
    "# esrgan_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "graduate-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(basepath+\"esrgan_hr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        filename = filename[:-3]\n",
    "        cur_filepath = os.path.join(basepath+\"esrgan_hr_noisy/\", filename+\".pt\")\n",
    "#         print(filename)\n",
    "#         img = ToTensor()(Image.open(cur_filepath))\n",
    "        img = torch.load(cur_filepath)\n",
    "        img = img.numpy()\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "        img = cv2.fastNlMeansDenoisingColored((img * 255).astype('uint8'), None,10,10,7,21)\n",
    "        img = torch.from_numpy(np.moveaxis(img, -1, 0).astype('float32'))\n",
    "        img = img / 255\n",
    "        torch.save(img, basepath + \"esrgan_cv2/\" + filename+\".pt\")\n",
    "        to_img(img).save(basepath+\"esrgan_cv2/\"+filename+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-trade",
   "metadata": {},
   "source": [
    "# srgan_didn TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adolescent-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/noisy-superres/data/DIV2K/vis_images/srgan_hr_noisy/img3.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.62 GiB (GPU 0; 11.17 GiB total capacity; 6.40 GiB already allocated; 2.51 GiB free; 8.17 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-59be2df6807b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenoise_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"srgan_didn/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/noisy-superres/DIDN/color_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mrecon6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecon1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.62 GiB (GPU 0; 11.17 GiB total capacity; 6.40 GiB already allocated; 2.51 GiB free; 8.17 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "for filename in os.listdir(basepath+\"srgan_hr_noisy/\"):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        with torch.no_grad():\n",
    "            filename = filename[:-3]\n",
    "            cur_filepath = os.path.join(basepath+\"srgan_hr_noisy/\", filename+\".pt\")\n",
    "            print(cur_filepath)\n",
    "            img = torch.load(cur_filepath)\n",
    "            dim = img.shape[-1] // 8 * 8\n",
    "            img = denoise_model(img[:,:dim, :dim].unsqueeze(0).to(device))\n",
    "            img = img[0].clip(0, 1)\n",
    "            torch.save(img, basepath + \"srgan_didn/\" + filename+\".pt\")\n",
    "            to_img(img).save(basepath+\"srgan_didn/\"+filename[:-3]+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-relation",
   "metadata": {},
   "source": [
    "# esrgan_didn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
